{
  "metadata": {
    "name": "Small_business_cleaning",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "from pyspark import SparkContext, SparkConf\ncf \u003d SparkConf()\ncf.set(\"spark.submit.deployMode\",\"client\")\nsc \u003d SparkContext.getOrCreate(cf)\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession \\\n\t    .builder \\\n\t    .appName(\"Python Spark SQL basic example\") \\\n\t    .config(\"spark.some.config.option\", \"some-value\") \\\n\t    .getOrCreate()\n                      "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Cleaning: Legally_Operating_Businesses_copy.csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Start by reading in \u0027Legally_Operating_Businesses_copy.csv\u0027 as a pandas data frame. We are going to be looking at the years and months, so it is important to change the datatypes of the date column to a datetime datatype. We will also change the datatypes of license status column, industry column, address state column, and address city column to string. This is so that all of the entries in this column are uniform. I will also modify the column names from words separated by spaces to words separated by underscores. For example \u0027Address State\u0027 will be renamed \u0027Address_State\u0027. We are only modifying certain columns because we are only interested in these columns of the dataset. All of this is done, so to make it easier to not only convert the dataframe into a pyspark dataframe, but also to make it easier to use SQL queries on the soon to be created pyspark dataframe."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\n#read dataset in pandas\n#set null values as a string, to make all the columns the same data type - makes it easier to convert to spark data frame\nbusiness_license \u003d pd.read_csv(\u0027Legally_Operating_Businesses_copy.csv\u0027, na_values \u003d \"not available\")\n\n#set respective columns to its designated datatype\nbusiness_license[\u0027License Creation Date\u0027] \u003d pd.to_datetime(business_license[\u0027License Creation Date\u0027]) \nbusiness_license \u003d business_license.astype({\u0027License Status\u0027:\u0027string\u0027,\u0027Industry\u0027:\u0027string\u0027, \u0027Address State\u0027:\u0027string\u0027, \u0027Address City\u0027:\u0027string\u0027 })\n\n#rename each column name from separate words to words separated by underscores\nbusiness_license \u003d business_license.loc[:, [\"License Creation Date\", \"License Status\", \"Industry\", \"Address State\", \"Address City\"]] \nbusiness_license.rename(columns\u003d{\"License Creation Date\": \"License_Creation_Date\", \"License Status\": \"License_Status\", \"Address State\":\"Address_State\", \"Address City\": \"Address_City\"}, inplace\u003dTrue)\n\nbusiness_license.dtypes"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \nRead the business_license pandas dataframe into a spark dataframe, This way we can now use Pyspark SQL queries to further work on the data"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "from pyspark.sql.types import *\n#create schema for your dataframe\nschema \u003d StructType([StructField(\"License_Creation_Date\", DateType(), True)\\\n                   ,StructField(\"License_Status\",StringType(), True)\\\n                   ,StructField(\"Industry\", StringType(), True)\\\n                   ,StructField(\"Address_State\", StringType(), True)\\\n                   ,StructField(\"Address_Borough\", StringType(), True)])\n\n#create spark dataframe using schema\nbusiness_license_df \u003d spark.createDataFrame(business_license,schema\u003dschema)\nbusiness_license_df.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "As we can see not all of the licenses are from New York, so I want to remove the entries of the table that are not businesses from NY. Also we are only focused on New York City, so we will only keep entries that are in the five boroughs (Queens, Manhattan/New York, Brooklyn, Bronx, Staten Island). There are some address boroughs that are labeled as cities within boroughs. We will not be including them because it will be nearly impossible to go through all the cities and match them to the correct borough. "
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#remove non NY entries\nbusiness_license_df \u003d business_license_df.filter( business_license_df[\"Address_State\"] \u003d\u003d \"NY\")\nbusiness_license_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#remove non NYC entries\n#In the dataset NEW YORK is the same as MANHATTAN, so I included both in my list of boroughs\nboroughs \u003d [\"QUEENS\", \"MANHATTAN\", \"NEW YORK\", \"STATEN ISLAND\", \"BROOKLYN\", \"BRONX\"]\nbusiness_license_df \u003d business_license_df.filter(business_license_df[\"Address_Borough\"].isin(boroughs))\nbusiness_license_df.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Now we want to look at the number of licenses created each year. We use a pyspark SQL query to do this. After, we then convert the SQL table to a new pyspark dataframe, which we futher convert back into a new pandas dataframe, so that we can export the table into a csv file. "
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "from pyspark.sql.functions import year\n\n#Use SQL to group the data by years and count number of licenses made\n\nbusiness_license_df.createOrReplaceTempView(\"license\")\nbusiness_license_by_year_df \u003d spark.sql(\"SELECT YEAR(License_Creation_Date) AS Year, COUNT(*) AS Opened_Licenses FROM license GROUP BY YEAR(License_Creation_Date) ORDER BY year\")\n\n#turn dataframe into pandas df to export as csv\nbusiness_license_by_year \u003d business_license_by_year_df.toPandas()\n#rename column back into words separated by spaces\nbusiness_license_by_year.rename(columns\u003d{\u0027Opened_Licenses\u0027: \u0027Opened Licenses\u0027}, inplace\u003dTrue)\ndisplay(business_license_by_year[business_license_by_year[\u0027Year\u0027] \u003e 2010])\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#export CSV file\nbusiness_license_by_year.to_csv(\u0027business_license_by_year_updated.csv\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We are doing the same thing here, except now we are grouping by Borough "
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#Use SQL to group the data by years and count number of licenses made\nbusiness_license_by_borough_df \u003d spark.sql(\"SELECT YEAR(License_Creation_Date) AS Year, Address_Borough AS Borough, COUNT(*) AS Opened_Licenses FROM license GROUP BY YEAR(License_Creation_Date), Address_Borough ORDER BY year\")\n#convert to pandas\nbusiness_license_by_borough \u003d business_license_by_borough_df.toPandas()\n#rename column\nbusiness_license_by_borough.rename(columns\u003d{\u0027Opened_Licenses\u0027: \u0027Opened Licenses\u0027}, inplace\u003dTrue)\ndisplay(business_license_by_borough[business_license_by_borough[\u0027Year\u0027] \u003e 2018])"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#export CSV\nbusiness_license_by_borough.to_csv(\u0027business_license_by_borough_updated.csv\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Once Again we repeat the same as above, except now we are grouping by industry"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#Use SQL to group by indsutry\nbusiness_license_by_ind_df \u003d spark.sql(\"SELECT YEAR(License_Creation_Date) AS Year, Industry, COUNT(*) AS Opened_Licenses FROM license GROUP BY YEAR(License_Creation_Date), Industry ORDER BY year DESC\")\n#convert to pandas\nbusiness_license_by_ind \u003d business_license_by_ind_df.toPandas()\n#rename\nbusiness_license_by_ind.rename(columns\u003d{\u0027Opened_Licenses\u0027: \u0027Opened Licenses\u0027}, inplace\u003dTrue)\ndisplay(business_license_by_ind[business_license_by_ind[\u0027Year\u0027] \u003e 2019])"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#export\nbusiness_license_by_ind.to_csv(\u0027business_license_by_industry_updated.csv\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Cleaning: cases-by-day.csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "First we start by reading the \u0027cases-by-day.csv\u0027 file into a pandas dataframe. We are doing this because we want to set the \u0027date_of_interest\u0027 column into a datetime datatype. This is so that later on when we convert it to a spark dataframe and try to perform SQL queries we can access that data by different dates. We then turn the pandas dataframe into a spark dataframe."
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#read csv as pandas\ncovid_numbers \u003d pd.read_csv(\"cases-by-day.csv\")\n#turn column into datetime datatype\ncovid_numbers[\u0027date_of_interest\u0027] \u003d pd.to_datetime(covid_numbers[\u0027date_of_interest\u0027])\n\n#convert to a spark dataframe and create a tempview for sql queries\ncovid_df \u003d spark.createDataFrame(covid_numbers)\ncovid_df.createOrReplaceTempView(\"covid\")\ncovid_df.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The covid dataset is taken per day. However, what we are trying to do is get an average number of cases per month. To do this we perform a SQL query where we group the data by year and then group by month. For each group we will take the average number of cases for that month. "
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "from pyspark.sql.functions import year, month, max,concat, col\ncovid_df_update \u003d spark.sql(\"SELECT YEAR(date_of_interest) AS year, MONTH(date_of_interest) AS month, AVG(ALL_CASE_COUNT_7DAY_AVG) AS cases FROM covid GROUP BY YEAR(date_of_interest), MONTH(date_of_interest) ORDER BY year, month\")\ncovid_df_update.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Another issue we are facing is that the year and month are now in separte columns. The next thing we do is combine the two columns. We start by converting the pyspark dataframe into a pandas dataframe. Next we append the year and month together and separting them by a period and store this as a new column in the dataframe. 2020 1 becomes 2020.1. Finally, we select only the cases column and the newly created year_month column. The purpose of combining the month and year columns is so that when we want to plot the data we can have both the year and month included in the plot."
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#convert to pandas\ncovid_numbers \u003d covid_df_update.toPandas()\n#combine year and month columns as a new column \u0027year_month\u0027\ncovid_numbers[\u0027year_month\u0027] \u003d covid_numbers[\u0027year\u0027].astype(str) + \u0027.\u0027 + covid_numbers[\u0027month\u0027].astype(str)\n#take only the cases column and \u0027year_month\u0027 column\ncovid_numbers \u003d covid_numbers.iloc[:,2:4]\ncovid_numbers"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#export as csv\ncovid_numbers.to_csv(\u0027covid_numbers_updated.csv\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Cleaning: savings.csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "I want to compare the data of covid cases to amount of personal savings. From the previous step, the \u0027covid_numbers_updated.csv\u0027 reads the months as \u00272020.1\u0027, \u00272020.2\u0027, etc. The Personal savings.csv does not group the data by this. In the  savings data, the columns are grouped by year in one row and by month in another row. In order to compare the two data sets I want to format the dates of the Personal savings data to match the updated covid numbers data. I start by reading the data into a pandas dataframe."
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import decimal\nincome_disposition \u003d pd.read_csv(\"savings.csv\")\nincome_disposition.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "As we can see, the first row of the table lists the months:\"JAN\",\"FEB\",etc. We also notice that the labels of the years already indicate the month with a number. Therefore, we do not really need the row with the months so we can delete that. We also don\u0027t need the first column of the dataset with the line count. We also notice that there is a row in the data that has empty values. We can either delete it or fill it in with 0\u0027s, in this case we fill it in with 0\u0027s. Also we want to make sure the data is the correct datatype, so we change every column to a float datatype. "
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#drop row with months and drop first column with row count\nincome_disposition \u003d income_disposition.drop([\u0027Line\u0027], axis \u003d 1)\nincome_disposition\u003d income_disposition.iloc[1:, :]\n\n#fill empty rows with 0\nincome_disposition \u003d income_disposition.fillna(0)\n\n#change column datatypes to floats\ncols \u003d income_disposition.columns\ncols \u003d cols[1:]\nfor col in cols:\n    income_disposition[col] \u003d income_disposition[col].astype(\"float\")\n\nincome_disposition.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Another thing we notice is that the years do not quite match up with year_month labels of our covid data. In our covid data, the year_month is represented as \"2020.1\" for January 2020, \"2020.2\" for February 2020, etc. However in the personal savings data, January 2020 is represented just as \"2020\" while February 2020 is \"2020.1. We can see that in our savings data, the years are off by a month. Therefore, when we try to compare the savings data with the covid numbers data, the dates will not line up. The last thing we do is fix this by iterating through each year label in the savings data and adjusting the year to match our covid numbers year_month. "
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#rename columns\n#create a list of the new column names\nnew_cols \u003d []\nfor i in range(len(income_disposition.columns)):\n  if i \u003d\u003d 0:\n    new_cols.append(\"type\")\n    continue\n\n  name \u003d decimal.Decimal(income_disposition.columns[i])\n  if income_disposition.columns[i][-2] \u003d\u003d \u0027.\u0027:\n    if(income_disposition.columns[i][-1] \u003d\u003d \u00279\u0027):\n      new_name \u003d income_disposition.columns[i].replace(\u0027.9\u0027, \u0027.10\u0027)\n    else:\n      new_name \u003d name + decimal.Decimal(\u00270.1\u0027)\n    new_cols.append(str(new_name))\n  elif income_disposition.columns[i][-3] \u003d\u003d \u0027.\u0027:\n    new_name \u003d name + decimal.Decimal(\u00270.01\u0027)\n    new_cols.append(str(new_name))\n  else:\n    new_name \u003d name + decimal.Decimal(\u00270.1\u0027)\n    new_cols.append(str(new_name))\n\n#replace the column names with new column names\nincome_disposition.columns \u003d new_cols\nincome_disposition.head()"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#export as csv\nincome_disposition.to_csv(\u0027savings_updated.csv\u0027)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Cleaning: us_small_bus.csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Here is a simple data set about the number of US small businesses over the years. It is a small dataset, that does not require much cleaning. However, there are small things that need to be fixed. We start by reading the dataset into a pandas dataframe. "
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "US_business_df \u003d pd.read_csv(\"us_small_bus.csv\")\nUS_business_df.head()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "As we can see the first column \u0027Line\u0027 is unecessary so we can drop it. There are also rows that are all 0\u0027s. These will also be unecessary so we can drop those rows as well. We can also see that the second column is named \u0027Unnamed:1\u0027 which is not very informative. We can rename this column to \u0027industry\u0027. Throughout the dataset, there are some strangely named entries such as \"Farms2\". Because the dataset is small, we can individually change the names of each of these entries. One last thing that is easy to overlook is that each entry in the \"Unnamed:1\" column (renamed to \"industry\") has leading white spaces. This will cause unecceasry issues later, so we can just remove all leading white space in each entry."
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#drop rows that are all 0\u0027s\nUS_business_df \u003d US_business_df.drop([0,6])\n\n#drop column \u0027Line\u0027 because it is not necessary\nUS_business_df \u003d US_business_df.drop(columns\u003d[\u0027Line\u0027])\n\n#rename the variables of two different cells\nUS_business_df.iloc[0,0] \u003d \"Self-employed persons\"\nUS_business_df.iloc[2,0] \u003d \"Farms\"\nUS_business_df.iloc[14,0] \u003d \"Professional and business services\"\n\n#rename first column\nUS_business_df.rename(columns\u003d{\u0027Unnamed: 1\u0027: \u0027industry\u0027}, inplace\u003dTrue)\n\n#clean up the leading white spaces in the industry column\nnum_industries \u003d len(US_business_df)\nfor i in range(num_industries):\n    cleaned_word \u003d US_business_df.iloc[i,0].strip()\n    US_business_df.iloc[i,0] \u003d cleaned_word\n\nUS_business_df.head()"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "#export as csv\nUS_business_df.to_csv(\u0027us_small_bus_updated.csv\u0027)"
    }
  ]
}